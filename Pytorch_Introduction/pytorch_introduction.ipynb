{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"pytorch_introduction.ipynb","provenance":[],"collapsed_sections":["1Oz_s2lRJHLP","HbpjypcLJHLe","lGM1kjsdJHLk","LcAhG9-1JHMI","MZxyPes0JHMw","YLiyhh3sJHM4","Mzkhg5D-JHNB"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z9Og8-UOJHKT"},"source":["(Optional) PyTorch Introduction\n","================\n"," \n","<div class=\"alert alert-info\">\n","    <strong>Note:</strong> This exercise is optional and only serves as an introduction and cheatsheet to the general concepts of PyTorch.\n","</div>\n","\n","PyTorch is a scientific computing package for Python:\n","\n","-  Tensor and Neural Network computations (in particular deep learning)\n","-  Research oriented (in comparison to e.g. TensorFlow)\n","-  Dynamic computational graph (in comparison to e.g. TensorFlow)\n","-  “NumPy on the GPU”\n","-  Backend and API heavily inspired by the original Torch written in Lua\n","\n","An in-depth tutorial of the concepts described in this notebook can be found [here](https://github.com/jcjohnson/pytorch-examples).\n","\n","Use the [pytorch website](https://pytorch.org/) and install the newest version."]},{"cell_type":"code","metadata":{"id":"p7mmk0s6YjRu","colab_type":"code","outputId":"1e2893e2-2b73-4336-aa0f-a842c731a793","executionInfo":{"status":"ok","timestamp":1579363433397,"user_tz":-60,"elapsed":1669,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"23DsqrOXYqZg","colab_type":"code","outputId":"f96dfd13-28cc-4060-8ee9-d8679e9907cd","executionInfo":{"status":"ok","timestamp":1579363433917,"user_tz":-60,"elapsed":2177,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd drive/My Drive/i2dl/PytorchIntroAndOptionalCNN"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/i2dl/PytorchIntroAndOptionalCNN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363438273,"user_tz":-60,"elapsed":6524,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"FkjMF9zNJHKV","outputId":"b555fa36-1ec8-4417-8696-904d841e92b2","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%matplotlib inline\n","import numpy as np\n","import torch\n","\n","print(torch.__version__)  # This should print 1.0.x"],"execution_count":4,"outputs":[{"output_type":"stream","text":["1.3.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JMaMwumpJHKb"},"source":["Tensors\n","=====\n","\n","The PyTorch `Tensor` class is very similar to the NumPy `ndarray` class. Their main distinction is the ability of PyTorch Tensors to be used on a GPU which lets them benefit from vastly accelerated and parallelized computations. In order to work with PyTorch it is crucial to understand the basic behavior of its `Tensor` class."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YiwhLXWaJHKc"},"source":["Let's start with the initialization of a regular `5x3` matrix `Tensor`:"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363438275,"user_tz":-60,"elapsed":6517,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"LWPT7G6uJHKe","outputId":"bed07b3f-521b-41a5-fdc5-69f0d9ad9ff8","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["x = torch.Tensor(5, 3)\n","print(x)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["tensor([[9.2858e-36, 0.0000e+00, 3.7835e-44],\n","        [0.0000e+00,        nan, 0.0000e+00],\n","        [1.3733e-14, 6.4069e+02, 4.3066e+21],\n","        [1.1824e+22, 4.3066e+21, 6.3828e+28],\n","        [3.8016e-39, 2.1749e+23, 0.0000e+00]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"U4vzgG4tJHKi"},"source":["The same matrix can be initialized with random entries:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363438276,"user_tz":-60,"elapsed":6510,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"Zjaw6crMJHKk","outputId":"fe5c4e83-80a0-4a24-f740-cf0017205620","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["x = torch.rand(5, 3)\n","print(x)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["tensor([[0.9039, 0.7340, 0.7250],\n","        [0.4571, 0.5199, 0.7768],\n","        [0.3266, 0.6783, 0.1148],\n","        [0.7041, 0.0210, 0.2536],\n","        [0.5103, 0.7978, 0.1930]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HlPiiRzTJHKo"},"source":["The size of a `Tensor` can be retrieved with:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sltKIup8JHKq","outputId":"7afa06d1-6a4c-47cb-ef37-f69849c7cc6e","executionInfo":{"status":"ok","timestamp":1579363438277,"user_tz":-60,"elapsed":6503,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(x.size())"],"execution_count":7,"outputs":[{"output_type":"stream","text":["torch.Size([5, 3])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2x2y3sDBJHKu"},"source":["<div class=\"alert alert-info\">\n","    <h3>Note</h3>\n","    <p>In contrast to a static computational graph of for example Tensorflow the dynamic graph of PyTorch allows to retrieve information such as its size at any time during runtime.</p>\n","</div>\n","\n","Tensor Operations\n","--------\n","\n","There are multiple syntaxes for `Tensor` operations. We illustrate the different options on the example of `Tensor` addition.\n","\n","Regular (NumPy) syntax:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363438278,"user_tz":-60,"elapsed":6496,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"H4cGcBOZJHKv","outputId":"31b11989-3deb-4f72-9d43-7b88e33db940","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["y = torch.rand(5, 3)\n","print(x + y)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["tensor([[0.9934, 1.4217, 1.0490],\n","        [0.8860, 0.7140, 1.0905],\n","        [1.2260, 1.5080, 0.2741],\n","        [0.9692, 0.1482, 1.0388],\n","        [0.5426, 0.9298, 0.2042]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9KA-L6icJHKz"},"source":["PyTorch syntax:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"N8Jtdw1GJHK1","outputId":"42b90b34-df6a-400a-eb07-68df22ba275e","executionInfo":{"status":"ok","timestamp":1579363438280,"user_tz":-60,"elapsed":6490,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["print(torch.add(x, y))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["tensor([[0.9934, 1.4217, 1.0490],\n","        [0.8860, 0.7140, 1.0905],\n","        [1.2260, 1.5080, 0.2741],\n","        [0.9692, 0.1482, 1.0388],\n","        [0.5426, 0.9298, 0.2042]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5h6ddHZcJHK5"},"source":["PyTorch syntax with specific output variable:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Lrtf7JkXJHK6","outputId":"dda7417b-0f2e-416e-9054-bce86b463cf3","executionInfo":{"status":"ok","timestamp":1579363438281,"user_tz":-60,"elapsed":6484,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["result = torch.Tensor(5, 3)\n","torch.add(x, y, out=result)\n","print(result)\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["tensor([[0.9934, 1.4217, 1.0490],\n","        [0.8860, 0.7140, 1.0905],\n","        [1.2260, 1.5080, 0.2741],\n","        [0.9692, 0.1482, 1.0388],\n","        [0.5426, 0.9298, 0.2042]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m0mpuxDLJHLC"},"source":["PyTorch syntax for inplace operations:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n5x61WcmJHLE","outputId":"2accd982-67c0-415e-877c-0c0fbd1cd7bf","executionInfo":{"status":"ok","timestamp":1579363438283,"user_tz":-60,"elapsed":6478,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# adds x to y\n","y.add_(x)\n","print(y)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["tensor([[0.9934, 1.4217, 1.0490],\n","        [0.8860, 0.7140, 1.0905],\n","        [1.2260, 1.5080, 0.2741],\n","        [0.9692, 0.1482, 1.0388],\n","        [0.5426, 0.9298, 0.2042]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kdSVbIvhJHLJ"},"source":["<div class=\"alert alert-info\">\n","    <h3>Note</h3>\n","    <p>Any operation that mutates a <code>Tensor</code> in-place is post-fixed with an <code>_</code>.</p>\n","    <p>For example: <code>x.t_()</code> (transposing x), <code>x.copy_(y)</code> (copy y to x).</p>\n","</div>\n","\n","`Tensor` indexing works just like standard NumPy indexing. And since recently PyTorch even supports `Tensor` [broadcasting](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html)!\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gkVqV0mEJHLL","outputId":"02a23167-aa82-4914-baa0-8782ef4ec748","executionInfo":{"status":"ok","timestamp":1579363438284,"user_tz":-60,"elapsed":6471,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(x[:, 0])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["tensor([0.9039, 0.4571, 0.3266, 0.7041, 0.5103])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1Oz_s2lRJHLP"},"source":["NumPy: There and back again\n","---------------------------\n","\n","Converting a PyTorch `Tensor` to a NumPy `ndarray` and vice versa is a very simple. The `Tensor` and the `ndarray` will share the location of the underlying memory, and changing one will also change the other.\n","\n","Converting a `Tensor` to a `ndarray` works by simply calling the `Tensor.numpy()` method:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2wLWiFgaJHLR","outputId":"927454d1-3556-472e-874b-aae46e10d56b","executionInfo":{"status":"ok","timestamp":1579363438285,"user_tz":-60,"elapsed":6464,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["a = torch.ones(5)\n","b = a.numpy()\n","print(a)\n","print(b)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["tensor([1., 1., 1., 1., 1.])\n","[1. 1. 1. 1. 1.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uYN_zpKiJHLU"},"source":["Changing the `Tensor` effects the `ndarray` as well:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gu3Nbjt1JHLV","outputId":"0f288b68-f724-4413-a306-4f0e0f20c15b","executionInfo":{"status":"ok","timestamp":1579363438287,"user_tz":-60,"elapsed":6458,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["a.add_(1)\n","print(a)\n","print(b)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["tensor([2., 2., 2., 2., 2.])\n","[2. 2. 2. 2. 2.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b6DEh45dJHLZ"},"source":["The conversion from a `ndarray` to a `Tensor` is just as simple and holds the same properties:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3hz_5DT-JHLa","outputId":"dda948df-9b03-4447-afd6-9eae764f54e6","executionInfo":{"status":"ok","timestamp":1579363438288,"user_tz":-60,"elapsed":6451,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["a = np.ones(5)\n","b = torch.from_numpy(a)\n","np.add(a, 1, out=a)\n","print(a)\n","print(b)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[2. 2. 2. 2. 2.]\n","tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HbpjypcLJHLe"},"source":["Every `Tensor` allocated on the CPU (except the `torch.CharTensor`) support converting to\n","NumPy and back.\n","\n","Tensors on the GPU\n","------------------\n","\n","PyTorch Tensors can be moved onto a GPU using the ``Tensor.to()`` method. Before converting a GPU `Tensor` to NumPy it has to be moved back to the CPU by calling the ``Tensor.to()`` method again.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363448821,"user_tz":-60,"elapsed":16976,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"nrfH4_ucJHLf","outputId":"8b035963-7656-4330-8112-fedbd8c1b61a","colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["# first check if cuda is available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","if device == torch.device(\"cuda:0\"):\n","    x = x.to(device)\n","    y = y.to(device)\n","    z = x + y\n","    \n","    print(z)\n","    print(z.cpu().detach().numpy())\n","else:\n","    print(\"CUDA not available.\")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["tensor([[1.8973, 2.1557, 1.7740],\n","        [1.3431, 1.2339, 1.8674],\n","        [1.5525, 2.1863, 0.3889],\n","        [1.6733, 0.1691, 1.2924],\n","        [1.0528, 1.7276, 0.3972]], device='cuda:0')\n","[[1.8972931  2.155666   1.7739854 ]\n"," [1.3430641  1.2338556  1.8673608 ]\n"," [1.5525291  2.186296   0.3888865 ]\n"," [1.6733133  0.16913372 1.2923894 ]\n"," [1.0528302  1.7276345  0.39716297]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lGM1kjsdJHLk"},"source":["More on PyTorch Tensors\n","-----------------------\n","\n","The documentation of many more `Tensor` operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers can be found [here](http://pytorch.org/docs/torch)."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"V6Oq8GCQJHLl"},"source":["\n","Autograd - automatic differentiation\n","===================================\n","\n","Central to all neural networks in PyTorch is the ``autograd`` package. The package provides automatic differentiation for all operations on Tensors. PyTorch is a define-by-run framework, which means that the calculation of gradients ( e.g. during backpropagation) is defined at runtime and can be different at every single iteration.\n","\n","Since pytorch 0.4.0, the Tensor class includes the Variable class, and supports nearly all of its operations. Once a computational graph for a Tensor that requires gradients is executed the ``Tensor.backward()`` method can be used to automatically compute all the gradients.\n","\n","If the ``Tensor`` is not a scalar, the ``backward()`` method requires an additional ``grad_output`` argument which matches the shape of the ``Tensor``. ``grad_output`` is supposed to be the gradient w.r.t the given output. For a scalar ``Tensor`` ``grad_output`` is assumed to be `torch.Tensor([1.0])`.\n","\n","The `autograd` package additionally provides a `Function` class which encodes a complete history of computation. Each `Tensor` with the `required_grad` attribute has a ``Tensor.grad_fn`` attribute which references the ``Function`` (e.g. an operation such as addition) that created the respective ``Tensor`` and thereby determines its gradient. For Tensors that were created by the user and not as a result of an operation the ``grad_fn`` attribute is ``None``.\n","\n","The following simple examples will illustrate the basic concepts of the ``autograd`` package."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363448822,"user_tz":-60,"elapsed":16970,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"OrCqvMVdJHLm","outputId":"7f186838-1813-4653-b5a5-a342e1d591a2","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# In general, tensors don't track gradients\n","x = torch.ones(1)\n","x.requires_grad"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363448823,"user_tz":-60,"elapsed":16962,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"8dSH9TMHJHLq","outputId":"12581b4d-ef2a-4dce-b852-684ecbbd81c8","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Thus we can't call the backward function on these tensors\n","try:\n","    x.backwards()\n","except AttributeError:\n","    print(\"Doesn't work...\")"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Doesn't work...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363448824,"user_tz":-60,"elapsed":16956,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"3zKTOTTpJHLv","outputId":"0b3509f3-5b27-4e5c-e365-71eb03867f48","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Enable gradient tracking\n","x = torch.ones((2, 2), requires_grad=True)\n","print(x)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dQMexId5JHL2"},"source":["Apply an operation to the `Tensor`:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363448825,"user_tz":-60,"elapsed":16949,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"C0N4O7cpJHL3","outputId":"c3456efd-ac72-4be1-a1e4-4daef2f1233b","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["y = x + 2\n","print(y)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["tensor([[3., 3.],\n","        [3., 3.]], grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UQ5XAznxJHL7"},"source":["Since ``y`` was created as a result of an operation it has a ``grad_fn`` attribute (`Function`) unequal to `None`:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363448826,"user_tz":-60,"elapsed":16943,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"z5OlT54CJHL8","outputId":"4cea983f-6aa2-4303-be30-7f9be24e5ff8","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(y.grad_fn)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["<AddBackward0 object at 0x7f0be321efd0>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VFyz6nP8JHMA"},"source":["Applying more operations to `y` increases the computational graph:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363448827,"user_tz":-60,"elapsed":16937,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"WjbZnJCQJHMB","outputId":"bb1c01e7-3dbe-42cd-abaf-bd7a8a08c66b","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["z = y * y * 3\n","out = z.mean()\n","\n","print(z)\n","print(out)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["tensor([[27., 27.],\n","        [27., 27.]], grad_fn=<MulBackward0>)\n","tensor(27., grad_fn=<MeanBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LcAhG9-1JHMI"},"source":["Gradients\n","---------\n","The gradient w.r.t the input `x` can now be computed (backpropagated) with ``out.backward()``. Remember for a scalar this is equivalent to doing ``out.backward(torch.Tensor([1.0]))``.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"V2IEtkzZJHMJ","colab":{}},"source":["out.backward()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6SpARC4iJHMN"},"source":["The input `x` was a `2x2` `Tensor` and therefore $\\frac{d(out)}{dx}$ yields a matrix with the same shape:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363448829,"user_tz":-60,"elapsed":16924,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"zpkMa5nAJHMO","outputId":"cc28cdfd-6a01-4bbc-af78-0240dd91f089","colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["print(x.grad)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pMbEChvWJHMS"},"source":["For such a small computation graph the solution can easily be verified:\n","\n","The output w.r.t. the input is given as \n","$$\n","\\begin{align}\n","    out =& \\frac{1}{4}\\sum_i z_i \\\\\n","        =& \\frac{1}{4}\\sum_i 3y_i y_i \\\\\n","        =& \\frac{1}{4}\\sum_i 3(x_i+2)^2\n","\\end{align}\n","$$.\n","\n","Therefore the gradient is $\\frac{\\partial out}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, which yields\n","$\\frac{\\partial out}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$ for a particular input $x_i=1$.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pDL-JBQhJHMX"},"source":["The `autograd` package in combination with the dynamic graph structure allow to do crazy things such as:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363448830,"user_tz":-60,"elapsed":16921,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"uj50FKDfJHMY","outputId":"950e4378-504a-4cdf-c485-f8c9cb4293f4","colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["x = torch.randn(3, requires_grad=True)\n","\n","y = x * 2\n","while y.norm() < 1000:\n","    y = y * 2\n","\n","print(y)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["tensor([-1023.4800,    29.7026,   187.2494], grad_fn=<MulBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bIsgfJ86JHMb"},"source":["\n","Neural Networks\n","===============\n","\n","The `Tensor` class in combination with the `autograd` package build the foundation for constructing Neural Networks (NNs) with PyTorch. To further fascilitate the construction and training of a NN the ``torch.nn`` package, which depends on `autograd` to define NN models and differentiate them, includes additional NN-specifc classes and helper functions.\n","\n","For example the ``nn.Module`` class which works as a boilerplate NN model class and eventually contains all the individual layers and the ``Module.forward(x)`` method that infers the input ``x`` and returns the output of a NN.\n","\n","The following is a grapical illustration of the infamous *LeNet* NN from Yann LeCun. This NN was trained to classify the MNIST dataset of handwritten digit images:\n","\n","It is a simple feed-forward network which takes the input, feeds it through several layers one after the other, and then finally produces the classification output.\n","\n","\n","Define *LeNet* with PyTorch\n","--------------------------\n","\n","The following is an example implementation of the classification network above: \n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363448830,"user_tz":-60,"elapsed":16914,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"TNpqRNdzJHMd","outputId":"94a62052-837d-44f1-a7f0-0cb81c4d942e","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["import torch\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class LeNet(nn.Module):\n","\n","    def __init__(self):\n","        \"\"\"\n","        Class constructor which preinitializes NN layers with trainable\n","        parameters.\n","        \"\"\"\n","        super(LeNet, self).__init__()\n","        # 1 input image channel, 6 output channels, 5x5 square convolution\n","        # conv kernel\n","        self.conv1 = nn.Conv2d(1, 6, 5)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forwards the input x through each of the NN layers and outputs the result.\n","        \"\"\"\n","        # Max pooling over a (2, 2) window\n","        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n","        # If the size is a square you can only specify a single number\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","        # An efficient transition from spatial conv layers to flat 1D fully \n","        # connected layers is achieved by only changing the \"view\" on the\n","        # underlying data and memory structure.\n","        x = x.view(-1, self.num_flat_features(x))\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","    def num_flat_features(self, x):\n","        \"\"\"\n","        Computes the number of features if the spatial input x is transformed\n","        to a 1D flat input.\n","        \"\"\"\n","        size = x.size()[1:]  # all dimensions except the batch dimension\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","        return num_features\n","\n","\n","net = LeNet()\n","print(net)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["LeNet(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=400, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MWyYiGTbJHMj"},"source":["Due to the `autograd` package a NN merely requires the definition of the ``Module.forward()`` method. The ``.backward()`` function (which backpropagtes the gradients) is automatically defined. Any `Tensor` operation is allowed in the ``forward`` function.\n","\n","The learnable parameters of a model are returned by ``Module.parameters()``:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363448831,"user_tz":-60,"elapsed":16908,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"9COyeSBeJHMl","outputId":"4398ee2b-2179-4cd4-e175-38e037aa8163","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["params = list(net.parameters())\n","print(len(params))\n","print(params[0].size())  # conv1's .weight"],"execution_count":27,"outputs":[{"output_type":"stream","text":["10\n","torch.Size([6, 1, 5, 5])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5R8uoVX-JHMo"},"source":["Check input and output"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363448832,"user_tz":-60,"elapsed":16902,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"IRuzjHtuJHMp","outputId":"0c6d8afc-18a6-427e-8466-7ea9ae0c4430","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["x = torch.randn((1, 1, 32, 32))\n","output = net(x)\n","print(output)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0904,  0.1222, -0.0414, -0.0550,  0.0796,  0.0053,  0.0827, -0.0661,\n","         -0.0681,  0.0977]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Mw_HtylVJHMs"},"source":["Before backpropagating for example a random gradient, the gradient buffers of all parameters should be set to zero:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p-8Eb8fXJHMt","colab":{}},"source":["net.zero_grad()\n","output.backward(torch.randn(1, 10))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MZxyPes0JHMw"},"source":["<div class=\"alert alert-info\">\n","    <h3>Note</h3>\n","    <p>Calling the <code>Tensor.backward()</code> method a second time before new inputs are forwarded will through an error. This is due to PyTorch deleting all the intermediary results in order to reduce memory consumption. Calling the <code>.backward()</code> method with the <code>retain_graph=True</code> argument keeps those results.\n","    </p>\n","</div>\n","\n","<div class=\"alert alert-info\">\n","    <h3>Note</h3>\n","    <p>The entire <code>torch.nn</code> package only supports inputs that are a mini-batch of samples, and not a single sample. For example, <code>nn.Conv2d</code> will take in a 4D Tensor of <code>nSamples x nChannels x Height x Width</code>. If you have a single sample, just use <code>x.unsqueeze(0)</code> to add a fake batch dimension.\n","    </p>\n","</div>\n","\n","Loss Function\n","-------------\n","A loss function takes the (output, target) pair as inputs, and computes a value that estimates \"how far away\" the output is from the target.\n","\n","There are several different loss functions predefined under the `torch.nn` package. An example of a simple loss is the ``nn.MSELoss`` which computes the mean-squared error between the input and the target value.\n","\n","More examples of predefined losses are documented [here](http://pytorch.org/docs/nn.html#loss-functions).\n","\n","A MSE loss example:"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1579363448834,"user_tz":-60,"elapsed":16856,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"id":"w64oErnTJHMx","outputId":"fa09fc8f-ce2e-4c96-cc9a-f0c0c6dcd379","colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["output = net(x)\n","target = torch.arange(1, 11).unsqueeze(0).float()  # a dummy target with 10 classes\n","criterion = nn.MSELoss()\n","\n","loss = criterion(output, target)\n","print(loss)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["tensor(38.3391, grad_fn=<MseLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1Okfs8rNJHM3"},"source":["When ``loss.backward()`` is called, the whole graph is differentiated w.r.t. the loss, and all Tensors with gradients in the graph will have their ``Tensor.grad`` attribute accumulated with the gradient."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YLiyhh3sJHM4"},"source":["Backpropagate the Loss\n","--------------------\n","\n","A curical step for optimizing the network weights is the backpropogation of the loss. The nature of a computational graph makes this as easy as calling ``loss.backward()``. But since the gradients will be accumulated to already existing gradients one has to clear them first."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3VSlvUyVJHM7","outputId":"36434daf-90ed-4240-b9ec-066d934e817f","executionInfo":{"status":"ok","timestamp":1579363448835,"user_tz":-60,"elapsed":16850,"user":{"displayName":"AshishK","photoUrl":"","userId":"06139417809117211416"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["net.zero_grad()     # zeroes the gradient buffers of all parameters\n","\n","print('conv1.bias.grad before backward')\n","print(net.conv1.bias.grad)\n","\n","loss.backward()\n","\n","print('conv1.bias.grad after backward')\n","print(net.conv1.bias.grad)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["conv1.bias.grad before backward\n","tensor([0., 0., 0., 0., 0., 0.])\n","conv1.bias.grad after backward\n","tensor([ 0.0219,  0.0505, -0.0817, -0.0329,  0.0181, -0.0577])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Mzkhg5D-JHNB"},"source":["Weights Optimization\n","------------------\n","The simplest update rule used in practice for optimizing the weights of a NN is the Stochastic Gradient Descent (SGD):\n","\n","``weight = weight - learning_rate * gradient``\n","\n","Like any other NN component the optimization step can be implemented with the basic PyTorch classes.\n","\n","For example:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y7dwv3VQJHNC","colab":{}},"source":["def sgd_step(net):\n","    learning_rate = 0.01\n","    for f in net.parameters():\n","        f.data.sub_(f.grad.data * learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1IhMYh2oJHNG"},"source":["However, the PyTorch framework contains a small optimization package called ``torch.optim``. It includes various predefined update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n","\n","<div class=\"alert alert-info\">\n","    <h3>Note</h3>\n","    <p>Common optimization options such as the L2-regularization (see <code>weight_decay</code> argument) are already included in the predefined optimization schemes.</p>\n","</div>\n","\n","Using it is very simple:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cxC_TMCkJHNH","colab":{}},"source":["import torch.optim as optim\n","\n","# create an optimizer\n","optimizer = optim.SGD(net.parameters(), lr=0.01, weight_decay=1e-3)\n","\n","# a single step of an example training loop\n","optimizer.zero_grad()   # zero the gradient buffers\n","output = net(x)\n","loss = criterion(output, target)\n","loss.backward()\n","optimizer.step()    # Does the update based on the accumalted gradients"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yT49Gs4XJHNM"},"source":["Recap\n","==============\n","\n","  -  ``torch.Tensor`` - A multi-dimensional array with a `requires_grad` option to record the history of operations applied to it.\n","  -  ``nn.Module`` - Neural network module. Convenient way of\n","     encapsulating parameters, with helpers for moving them to GPU,\n","     exporting, loading, etc.\n","  -  ``nn.Parameter`` - A kind of `Tensor`, that is automatically\n","     registered as a parameter when assigned as an attribute to a\n","     ``Module``.\n","  -  ``autograd.Function`` - Implements forward and backward definitions\n","     of an autograd operation. Every ``Tensor`` operation that requires gradients, creates at\n","     least a single ``Function`` node, that connects to functions that\n","     created a ``Tensor`` and encodes its history.\n","\n","<div class=\"alert alert-info\">\n","    <h3>Note</h3>\n","    <p>The `torchvision` package includes many predefined helper funcitons specifally designed for solving computer vision problems.</p>\n","</div>"]}]}